{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of WorkingWithData.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vwlr40e88m-"
      },
      "source": [
        "# Working with Data\n",
        "\n",
        "#### Part of the [Inquiryum Machine Learning Fundamentals Course](http://inquiryum.com/machine-learning/)\n",
        "\n",
        "In the examples we have been working with so far, all the columns had numerical data. For example, the violet classification data looked like:\n",
        "    \n",
        "    \n",
        "Sepal Length|Sepal Width|Petal Length|Petal Width|Class\n",
        ":--: | :--: |:--: |:--: |:--: \n",
        "5.3|3.7|1.5|0.2|Iris-setosa\n",
        "5.0|3.3|1.4|0.2|Iris-setosa\n",
        "5.0|2.0|3.5|1.0|Iris-versicolor\n",
        "5.9|3.0|4.2|1.5|Iris-versicolor\n",
        "6.3|3.4|5.6|2.4|Iris-virginica\n",
        "6.4|3.1|5.5|1.8|Iris-virginica\n",
        "\n",
        "Notice that all the feature columns had numeric data. This isn't always the case. In addition to **numeric** data, datasets often contain **categorical data**. A column that contains **categorical data** means that the values are from a limited set of values. For example:\n",
        "\n",
        "\n",
        "\n",
        "Movie | Tomato Rating | Genre | Rating | Length \n",
        ":---: | :---: | :---: | :---: | :---:  \n",
        "First Man | 88 | Drama | PG-13 | 138\n",
        "Can You Ever Forgive Me | 98 | Drama | R | 107\n",
        "The Girl in the Spider's Web | 41 | Drama | R | -99\n",
        "Free Solo | 99 | Documentary | PG-13 | 97\n",
        "The Grinch | 57 | Animation | PG | 86\n",
        "Overlord | 80 | Action | R | 109\n",
        "Christopher Robin | 71 | Comedy | PG | -99\n",
        "Ant Man and the Wasp  |  88 | Science Fiction | PG-13 | 118\n",
        "\n",
        "Numeric columns like `Tomato Rating` and `Length` are fine as is, but the columns `Genre` and `Rating` are problematic for machine learning. Those columns contain categorical data which again means that the values of those columns are from a limited set of possibilities. Modern machine learning algorithms are designed to handle only numeric and boolean (True, False) data. So, as a preprocessing step, we will need to convert the categorical columns to numeric. One solution would be simply to map each categorical value to an integer. So drama is 1, documentary 2 etc:\n",
        "\n",
        "index | genre\n",
        " :--: | :--:\n",
        " 1 | Drama\n",
        " 2 | Documentary\n",
        " 3 | Animation\n",
        " 4| Action\n",
        " 5 | Comedy\n",
        " 6 | Science Fiction\n",
        "\n",
        "Using this scheme we can convert the original data to:\n",
        "\n",
        "Movie | Tomato Rating | Genre | Rating | Length \n",
        ":---: | :---: | :---: | :---: | :---:  \n",
        "First Man | 88 | 1 | 1 | 138\n",
        "Can You Ever Forgive Me | 98 | 1 | 2 | 107\n",
        "The Girl in the Spider's Web | 41 | 1 | 2 | -99\n",
        "Free Solo | 99 | 2 | 1 | 97\n",
        "The Grinch | 57 | 3 | 3 | 86\n",
        "Overlord | 80 | 4 | 2 | 109\n",
        "Christopher Robin | 71 | 5 | 3 | -99\n",
        "Ant Man and the Wasp  |  88 | 6 | 1 | 118\n",
        "\n",
        "\n",
        "But this solution is problematic in a different way. Integers infer  both an ordering and a distance where 2 is closer to 1 than 4. Since in the genre column 1 is drama, 2 is documentary, and 4 is action, our scheme implies that dramas are closer to documentaries than they are to action films, which is clearly not the case. This problem also exists in the rating column. Mapping the categories to integers in a different way will not fix this problem. No matter how clever we are in making this mapping, the problem will still exist. **So clearly this method is not the way to go**!\n",
        "\n",
        "### One Hot Encoding\n",
        "The solution is to do what is called one hot encoding. Our original table looked like:\n",
        "\n",
        "\n",
        "Movie | Tomato Rating | Genre | Rating | Length \n",
        ":---: | :---: | :---: | :---: | :---:  \n",
        "First Man | 88 | Drama | PG-13 | 138\n",
        "Can You Ever Forgive Me | 98 | Drama | R | 107\n",
        "The Girl in the Spider's Web | 41 | Drama | R | -99\n",
        "Free Solo | 99 | Documentary | PG-13 | 97\n",
        "The Grinch | 57 | Animation | PG | 86\n",
        "Overlord | 80 | Action | R | 109\n",
        "Christopher Robin | 71 | Comedy | PG | -99\n",
        "Ant Man and the Wasp  |  88 | Science Fiction | PG-13 | 118\n",
        "\n",
        "So, for example, we had the categorical column genre with the possible values drama, documentary, animation, action, comedy and science fiction. Instead of one column with those values, we are going to convert it to a form where each value is its own column.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/normalize2.jpg)\n",
        "\n",
        "\n",
        "If that data instance is of that value then we would put a **one** in the column, otherwise we would put a zero. For example, since *The Girl in the Spider's Web* is a drama, we would put a 1 in the drama column and a zero in the animation column. So we would convert\n",
        "\n",
        "Movie | Genre \n",
        ":---: | :---: \n",
        "First Man | Drama \n",
        "Can You Ever Forgive Me | Drama\n",
        "The Girl in the Spider's Web |  Drama \n",
        "Free Solo |  Documentary \n",
        "The Grinch |  Animation \n",
        "Overlord |  Action\n",
        "Christopher Robin |  Comedy\n",
        "Ant Man and the Wasp  |   Science Fiction\n",
        "\n",
        "to\n",
        "\n",
        "Movie | Drama | Documentary | Animation | Action | Comedy | Science Fiction\n",
        ":--: | :--: | :--: | :--: | :--: | :--: | :--: \n",
        "First Man | 1 | 0 | 0| 0| 0 | 0 \n",
        "Can You Ever Forgive Me | 1 | 0 | 0| 0| 0 | 0 \n",
        "The Girl in the Spider's Web | 1 | 0 | 0| 0| 0 | 0 \n",
        "Free Solo | 0 | 1 | 0| 0| 0 | 0 \n",
        "The Grinch | 0 | 0 | 1| 0| 0 | 0 \n",
        "Overlord | 0 | 0 | 0| 1| 0 | 0 \n",
        "Christopher Robin | 0 | 0 | 0| 0| 1 | 0 \n",
        "Ant Man and the Wasp | 0 | 0 | 0| 0| 0 | 1 \n",
        "\n",
        "Notice that the movie *First Man* has a one in the drama column and zeroes elsewhere. The movie *Free Solo* has a one in the documentary column and zeroes elsewhere.\n",
        "This is the prefered way of converting categorical data (when we work with text we will see other options). An added benefit to this approach is now an instance can be of multiple categories. For example, we may want to categorize *Ant Man and the Wasp* as both a comedy and science fiction, and that is easy to do in this scheme:\n",
        "\n",
        "Movie | Drama | Documentary | Animation | Action | Comedy | Science Fiction\n",
        ":--: | :--: | :--: | :--: | :--: | :--: | :--: \n",
        "First Man | 1 | 0 | 0| 0| 0 | 0 \n",
        "Can You Ever Forgive Me | 1 | 0 | 0| 0| 0 | 0 \n",
        "The Girl in the Spider's Web | 1 | 0 | 0| 0| 0 | 0 \n",
        "Free Solo | 0 | 1 | 0| 0| 0 | 0 \n",
        "The Grinch | 0 | 0 | 1| 0| 0 | 0 \n",
        "Overlord | 0 | 0 | 0| 1| 0 | 0 \n",
        "Christopher Robin | 0 | 0 | 0| 0| 1 | 0 \n",
        "Ant Man and the Wasp | 0 | 0 | 0| 0| 1 | 1 \n",
        "\n",
        "\n",
        "\n",
        "If we one-hot encoded all the categorical columns in our original dataset it would look like:\n",
        "\n",
        "Movie            | Tomato Rating | Action | Animation | Comedy | Documentary | Drama | Science Fiction | PG | PG-13 | R | Length \n",
        ":---: | :---: | :---: | :---: | :---: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: |  :---: \n",
        "First Man        | 88            |  0     |    0      |   0    | 0           | 1     | 0    | 0 | 1  |    0| 138\n",
        "Can You Ever Forgive Me | 98 |      0     |    0      |   0    | 0           | 1     | 0    | 0 | 0  |   1|   107\n",
        "The Girl in the Spider's Web | 41 |  0     |    0      |   0    | 0           | 1     | 0    | 0 | 1  |    0|    -99\n",
        "Free Solo | 99 |  0     |    0      |   0    | 1           | 0     | 0    | 0 | 1  |    0|   97\n",
        "The Grinch | 57 |  0     |    1      |   0    | 0           | 0     | 0    | 1 | 0  |    0| 86\n",
        "Overlord | 80 |  1     |    0      |   0    | 0           | 0     | 0    | 0 | 1  |    0|  109\n",
        "Christopher Robin | 71  |  0     |    0      |   1   | 0           | 0     | 0    | 1 | 0  |    0| -99\n",
        "Ant Man and the Wasp   |  0    |    0      |   0    | 0       | 0    | 0     | 1    | 0 | 1  |    0| 118\n",
        "\n",
        "\n",
        "\n",
        "### Coding\n",
        "Let's investigate this a bit with a coding example. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-vKAhtE88nA",
        "outputId": "3a407c58-09c9-4742-9a18-2df23764803a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "import pandas as pd\n",
        "bike = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/bike.csv')\n",
        "bike = bike.set_index('Day')\n",
        "bike\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Outlook</th>\n",
              "      <th>Temperature</th>\n",
              "      <th>Humidity</th>\n",
              "      <th>Wind</th>\n",
              "      <th>Bike</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Day</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Overcast</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Rain</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Rain</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Overcast</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Sunny</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Sunny</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Sunny</td>\n",
              "      <td>Mild</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Overcast</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Overcast</td>\n",
              "      <td>Hot</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Outlook Temperature Humidity    Wind Bike\n",
              "Day                                            \n",
              "1       Sunny         Hot     High    Weak   No\n",
              "2       Sunny         Hot     High  Strong   No\n",
              "3    Overcast         Hot     High    Weak  Yes\n",
              "4        Rain        Mild     High    Weak  Yes\n",
              "5        Rain        Cool   Normal    Weak  Yes\n",
              "6        Rain        Cool   Normal  Strong   No\n",
              "7    Overcast        Cool   Normal  Strong  Yes\n",
              "8       Sunny        Mild     High    Weak   No\n",
              "9       Sunny        Cool   Normal    Weak  Yes\n",
              "10       Rain        Mild   Normal    Weak  Yes\n",
              "11      Sunny        Mild   Normal  Strong  Yes\n",
              "12   Overcast        Mild     High  Strong  Yes\n",
              "13   Overcast         Hot   Normal    Weak  Yes\n",
              "14       Rain        Mild     High  Strong   No"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPws--z88nK"
      },
      "source": [
        "Here we are trying to predict whether someone will mountain bike or not based on the outlook, temperature, humidity, and wind. \n",
        "Let's forge ahead and see if we can build a decision tree classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSdLzEVb88nO",
        "outputId": "ced1a02c-ed4f-436d-b6a1-9d150e5b6d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(bike[['Outlook', 'Temperature', 'Humidity', 'Wind']], bike['Bike'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-345cbf4908d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Outlook'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Temperature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Humidity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Wind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bike'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Sunny'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7izTB-rr88nW"
      },
      "source": [
        "And we see that doesn't work. We get the error:\n",
        "\n",
        "```\n",
        "ValueError: could not convert string to float: 'Sunny'\n",
        "```\n",
        "\n",
        "We need to one-hot encode these categorical columns.  Here is how to convert the Outlook column. The steps are\n",
        "\n",
        "1. Create a new Dataframe of the one-hot encoded values for the Outlook column.\n",
        "2. Drop the Outlook column from the original Dataframe.\n",
        "3. Join the new one-hot encoded Dataframe to the original.\n",
        "\n",
        "#### 1. Create the new Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxYRQvrw88nX"
      },
      "source": [
        "one_hot = pd.get_dummies(bike['Outlook'])\n",
        "one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ctwuhS88nd"
      },
      "source": [
        "Nice.\n",
        "\n",
        "#### 2. Drop the outlook column from the original Dataframe:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0yvEY4e88ne"
      },
      "source": [
        "bike = bike.drop('Outlook', axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-f72v3188nl"
      },
      "source": [
        "#### 3. join the one-hot encoded Dataframe to the original"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQaNL7gQ88nl"
      },
      "source": [
        "bike = bike.join(one_hot)\n",
        "bike"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3zB9b6a88nt"
      },
      "source": [
        "It is simple, but a little tedious. Let's finish up encoding the other columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSTyuRJj88nu"
      },
      "source": [
        "one_hot = pd.get_dummies(bike['Temperature'])\n",
        "bike = bike.drop('Temperature', axis=1)\n",
        "bike = bike.join(one_hot)\n",
        "one_hot = pd.get_dummies(bike['Humidity'])\n",
        "bike = bike.drop('Humidity', axis=1)\n",
        "bike = bike.join(one_hot)\n",
        "one_hot = pd.get_dummies(bike['Wind'])\n",
        "bike = bike.drop('Wind', axis=1)\n",
        "bike = bike.join(one_hot)\n",
        "\n",
        "bike"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvUXQ6cZ88nz"
      },
      "source": [
        "Great! Now we can train our classifier. I will just cut and paste the previous `clf.fit` and ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sVlk6Y988n0"
      },
      "source": [
        "clf.fit(bike[['Outlook', 'Temperature', 'Humidity', 'Wind']], bike['Bike'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJDWZ-Et88n5"
      },
      "source": [
        "Well that didn't work. The clf.fit instruction was\n",
        "\n",
        "```\n",
        "clf.fit(bike[['Outlook', 'Temperature', 'Humidity', 'Wind']], bike['Bike'])\n",
        "```\n",
        "So we instruct it to use the Outlook, Temperature, Humidity, and Wind columns, but we just deleted them. Instead we have the following columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRB_hHrt88n6"
      },
      "source": [
        "list(bike.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtoT3ZKi88oA"
      },
      "source": [
        "Using that list let's divide up our data into the label (what we are trying to predict) and the features (what we are using to make the prediction)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCLwReH88oB"
      },
      "source": [
        "fColumns = list(bike.columns)\n",
        "fColumns.remove('Bike')\n",
        "bike_features = bike[fColumns]\n",
        "bike_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFgKuIt788oI"
      },
      "source": [
        "and now the label:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ABdDZ5Ni88oI"
      },
      "source": [
        "bike_labels = bike[['Bike']]\n",
        "bike_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP9iOylh88oP"
      },
      "source": [
        "Now, finally, we can train our decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1fbXI8Z88oP"
      },
      "source": [
        "clf.fit(bike_features, bike_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08dcdpts88oY"
      },
      "source": [
        "As you can see, preparing the data, can actually take a longer time than running the machine learning component.\n",
        "\n",
        "### `get_dummies` not the only way\n",
        "\n",
        "There are other methods to one-hot encode a dataset. For example, sklearn has a class, [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encorder), which might be a better option for many machine learning tasks. The reason I selected `get_dummies` for this notebook was a pedagogical one---`get_dummies` is a bit more transparent and you get a better sense of what one-hot-encoding does.\n",
        "\n",
        "### Conditionals for munging data\n",
        "\n",
        "Let's say we have this small DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo-bKWpc88oZ"
      },
      "source": [
        "from pandas import DataFrame\n",
        "students = DataFrame({'name': ['Ann', 'Ben', 'Clara', 'Danielle', 'Eric', 'Akash'],\n",
        "                     'sex':   ['f', 'm', 'f', 'f', 'm', 'm'],\n",
        "                     'age': [21, 18, 23, 19, 20, 21]})\n",
        "students"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id9BZykt88oe"
      },
      "source": [
        "The column sex is categorical so we need to convert it. We could \n",
        "* one-hot-encode it and have two columns: f and m. \n",
        "* one-hot-encode it, have two columns: f and m, and then delete one of those columns\n",
        "* create a column female and populate it correctly using a conditional\n",
        "\n",
        "All three are fine options with the last 2 slightly better since they reduce the dimensionality. Let's see how we can do the last one using a lambda expression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5i-r1TO88of"
      },
      "source": [
        "students['female'] =  students['sex'].apply(lambda x: True if x == 'f' else False)\n",
        "students  = students.drop('sex', axis=1) \n",
        "students"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR6DNRvT88ol"
      },
      "source": [
        "That's great! \n",
        "\n",
        "Now suppose we think that whether or not a person is under 20 is relevant for our machine learning task. We can use the same type of lambda expression to create this new column:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0FhFOXr88ol"
      },
      "source": [
        "students['under20'] =  students['age'].apply(lambda x: True if x < 20 else False)\n",
        "students"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXJ4j77k88oq"
      },
      "source": [
        "As you can see, working with machine learning involves working on a pipeline of various processes--we don't start with the machine learning algorithm. Before leaping into the ML algorithm, takes some time to explore the data, decide if it needs to be cleaned in any way, one-hot-encoded, if some features are not needed or if new ones need to be added.\n",
        "\n",
        "## Hyperparameters.\n",
        "\n",
        "When we train a machine learning model (using `fit` in this case), the model learns a set of **parameters**. For example, in decision trees, one parameter is the depth of the tree. The depth isn't determined until the `fit` method finishes. The important point is that parameters are what the model learns on its own from analyzing the training dataset and not something we adjust.\n",
        "\n",
        "In contrast **hyperparameters** are things we determine and not determined by the algorithm. Hyperparameters are set before the model looks at the training data--in our case before `fit`.  For decision trees there are a number of these hyperparameters. We already saw two: `max_depth` controls the size of the tree and `criterion`. Adjusting one hyperparameter may improve the accuracy of your classifier or it may worsen it. \n",
        "\n",
        "We have already learned that we shouldn't test our model using the same data that we trained on. Why not? Because the model is already tuned to the specific instances in our training data. In a kNN classifier, it may memorize every instance in our training data--*Gabby Douglas who is 49 inches tall and weighs 90 pounds is a gymnast*.  If we test using that same data, the accuracy will tend to be higher than if we tested using data the classifier has never seen before. Again, if we told the algorithm someone who is 49 inches tall and 90 pounds is a gymnast, we shouldn't find it surprising that if we asked it what sport does someone play who is 49 inches and 90 pounds, and the algorithm predicts *gymnast*. We want to see if the algorithm learned or generalized something from processing the dataset. In some previous labs, we reserved 20% of the original data to test on and used 80% for training. \n",
        "\n",
        "Now let's imagine a process where we will adjust hyperparameters to improve the accuracy of our model. So we build a classifier with one setting of the hyperparameters and build another with a different setting for the hyperparameters and see which one is more accurate. One approach might be:\n",
        "\n",
        "1. Use 80% of the data to train on.\n",
        "2. Test the classifier using the 20% test set and get the accuracy.\n",
        "3. Adjust a hyperparameter and create a new classifier\n",
        "4. Use the same 80% of the data to train the new classifier\n",
        "5. Test the classifier using the 20% test set and get the accuracy.\n",
        "6. Keep repeating this to find the value of the hyperparameter that performs the best.\n",
        "7. The accuracy of your classifier will be the highest one obtained from evaluating the 20% test set.\n",
        "\n",
        "The problem with this approach is that since we are tuning the hyperparameters based on the accuracy on the test set, some of the information about the test set is leaking into our classifier. Let me explain about information leaking into the classifier.\n",
        "\n",
        "Let's look at our example of categorizing athletes into one of three categories: gymnast, basketball player, and marathoner. Leilani\n",
        "Mitchell is not in the training set but is in the test set. She is 5 foot 5 inches tall and weighs 138. Initially, she was among the instances in the test set that were misclassified. We kept adjusting the hyperparameters until we improved accuracy and now she is correctly classified as a basketball player. So we tuned our classifier to work well with her and others in the test set. That is what we mean by information from the test set leaking into the classifier.\n",
        "\n",
        "So again, we may get an arbitrary higher accuracy that is not reflective of the algorithm's performance on unseen data.\n",
        "\n",
        "**So what can we do?**\n",
        "\n",
        "The solution is to divide the original dataset into three:\n",
        "\n",
        "1. the training set which we use to train our model.\n",
        "2. the validation set which we use to test our model so we can adjust the hyperparameters\n",
        "3. the test set which we use to perform an evaluation of the final model fit on the training set. We make our final adjustment of the hyperparameters **before** we evaluate the model using the test set.\n",
        "\n",
        "There are many ways to divide up the original data into these three sets. For example, maybe 20% is reserved for the test set, 20% for validation and 60% for training.  However, there is a slightly better way.\n",
        "\n",
        "### Cross Validation\n",
        "For cross validation we are going to divide the dataset (typically just the training dataset) into roughly equal sized buckets. Typically we would divide the data into 10 parts and this is called 10-fold cross validation. To reiterate, with this method we have one data set which we divide randomly into 10 parts. We use 9 of\n",
        "those parts for training and reserve one tenth for validation. We repeat this procedure 10 times\n",
        "each time reserving a different tenth for validation.\n",
        "\n",
        "Let’s look at an example. Suppose we want to build a classifier that just answers yes or no to\n",
        "the question *Is this person a professional basketball player?* And our data consists of information\n",
        "about 500 basketball players and 500 non-basketball players. \n",
        "\n",
        "#### Step 1. Divide the data into 10 bucks.\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/buckets.png)\n",
        "We put 50 basketball players and 50 non-players in each bucket so each bucket contains information on 100 individuals.\n",
        "\n",
        "#### Step 2. We iterate through the following steps 10 times\n",
        "\n",
        "1. During each iteration hold back one of the buckets. For iteration 1, we will hold back bucket 1, iteration 2, bucket 2, and so on.\n",
        "2. We will train the classifier with data from the other buckets. (during the first iteration we will train with the data in buckets 2 through 10)\n",
        "3. We will validate the classifier we just built using data from the bucket we held back and save the results. In our case these results might be: 35 of the basketball players were classified correctly and  29 of the non basketball players were classified correctly. \n",
        "\n",
        "\n",
        "#### Step 3. we sum up the results.\n",
        "Once we finish the ten iterations we sum the results. Perhaps we find that 937 of the 1,000 individuals were categorized correctly. \n",
        "\n",
        "#### Summary\n",
        "Using cross-validation, every instance in our data is used in training and, in a different iteration, in validation. This results in a less biased model. By **bias** we mean that the algorithm is less accurate due to it not taking into account all relevant information in the data. With cross-validation we typically train on a larger percentage of the data than we would if we set aside a fixed validation set. One small disadvantage is that it now take 10 times as long to run.\n",
        "\n",
        "\n",
        "### Leave One Out\n",
        "Here is a suggestion from Lucy:\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/nfold.png)\n",
        "\n",
        "In the machine learning literature, n-fold cross validation (where n is the number of samples\n",
        "in our data set) is called leave-one-out. Lucy above, already mentioned one benefit of leave-one-out—\n",
        "at every iteration we are using the largest possible amount of our data for training. The other\n",
        "benefit is that it is deterministic.\n",
        "\n",
        "#### What do we mean by ‘deterministic’?\n",
        "\n",
        "Suppose Lucy spends an intense 80 hour week creating and coding a new classifier. It is\n",
        "Friday and she is exhausted so she asks two of her colleagues (Emily and Li) to evaluate the\n",
        "classifier over the weekend. She gives each of them the classifier and the same dataset and\n",
        "asks them to use 10-fold cross validation. On Monday she asks for the results ..\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/nfoldwomen2.png)\n",
        "\n",
        "Hmm. They did not get the same results. Did Emily or Li make a mistake? Not necessarily. In\n",
        "10-fold cross validation we place the data randomly into 10 buckets. Since there is this\n",
        "random element, it is likely that Emily and Li did not divide the data into buckets in exactly\n",
        "the same way. In fact, it is highly unlikely that they did. So when they train the classifier, they\n",
        "are not using exactly the same data and when they test this classifier they are using different\n",
        "test sets. So it is quite logical that they would get different results. This result has nothing to\n",
        "do with the fact that two different people were performing the evaluation. If Lucy herself ran\n",
        "10-fold cross validation twice, she too would get slightly different results. The reason we get\n",
        "different results is that there is a random component to placing the data into buckets. So 10-\n",
        "fold cross validation is called non-deterministic because when we run the test again we are\n",
        "not guaranteed to get the same result. In contrast, the leave-one-out method is deterministic.\n",
        "Every time we use leave-one-out on the same classifier and the same data we will get the\n",
        "same result. That is a good thing!\n",
        "\n",
        "#### The disadvantages of leave-one-out\n",
        "The main disadvantage of leave-one-out is the computational expense of the method.\n",
        "Consider a modest-sized dataset of 10,000 instances and that it takes one minute to train a\n",
        "classifier. For 10-fold cross validation we will spend 10 minutes in training. In leave-one-out\n",
        "we will spend 16 hours in training. If our dataset contains 10 million entries the total time\n",
        "spent in training would nearly be two years. Eeeks!\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/twoyears.png)\n",
        "\n",
        "\n",
        "The other disadvantage of leave-one-out is related to stratification.\n",
        "\n",
        "\n",
        "\n",
        "#### Stratification.\n",
        "Let us return to the example of building a classifier that predicts\n",
        "what sport a woman plays (basketball, gymnastics, or track). When training the classifier we\n",
        "want the training data to be representative and contain data from all three classes. Suppose\n",
        "we assign data to the training set in a completely random way. It is possible that no\n",
        "basketball players would be included in the training set and because of this, the resulting\n",
        "classifier would not be very good at classifying basketball players. Or consider creating a data\n",
        "set of 100 athletes. First we go to the Women’s NBA website and write down the info on 33\n",
        "basketball players; next we go to Wikipedia and get 33 women who competed in gymnastics\n",
        "at the 2012 Olympics and write that down; finally, we go again to Wikipedia to get\n",
        "information on women who competed in track at the Olympics and record data for 34 people.\n",
        "So our dataset looks like this:\n",
        "\n",
        "![](https://raw.githubusercontent.com/zacharski/ml-class/master/labs/pics/womensports.png)\n",
        "\n",
        "Let’s say we are doing 10-fold cross validation. We start at the beginning of the list and put\n",
        "every ten people in a different bucket. In this case we have 10 basketball players in both the\n",
        "first and second buckets. The third bucket has both basketball players and gymnasts. The\n",
        "fourth and fifth buckets solely contain gymnasts and so on. None of our buckets are\n",
        "representative of the dataset as a whole and you would be correct in thinking this would skew\n",
        "our results. The preferred method of assigning instances to buckets is to make sure that the\n",
        "classes (basketball players, gymnasts, marathoners) are represented in the same proportions\n",
        "as they are in the complete dataset. Since one-third of the complete dataset consists of\n",
        "basketball players, one-third of the entries in each bucket should also be basketball players.\n",
        "And one-third the entries should be gymnasts and one-third marathoners. This is called\n",
        "stratification and this is a good thing. The problem with the leave-one-out evaluation\n",
        "method is that necessarily all the test sets are non-stratified since they contain only one\n",
        "instance. In sum, while leave-one-out may be appropriate for very small datasets, 10-fold\n",
        "cross validation is by far the most popular choice.\n",
        "\n",
        "### Coding\n",
        "Let's see how we can use cross validation using the Iris dataset.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1e/IMG_7911-Iris_virginica.jpg\" width=\"250\" />\n",
        "\n",
        "First, let's load the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSddqwD788or"
      },
      "source": [
        "iris = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/iris.csv')\n",
        "iris\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9K4_hbX88ov"
      },
      "source": [
        "Now let's divide this into a training set and a test set using an 80-20 split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEsmM_0z88ox"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "iris_train, iris_test = train_test_split(iris, test_size = 0.2)\n",
        "iris_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZWgFEcw88o1"
      },
      "source": [
        "#### 10 fold cross validation on iris_train\n",
        "First, to make things as clear as possible, we will split the iris_train dataset into the features and the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5L8QY6X88o2"
      },
      "source": [
        "iris_train_features = iris_train[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
        "iris_train_labels = iris_train[['Class']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYfLkXIV88o5"
      },
      "source": [
        "Let's also create an instance of a decision tree classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgtAfxcY88o7"
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_pOLjni88o-"
      },
      "source": [
        "### The cross validation steps\n",
        "\n",
        "#### Step 1. Import  cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWhV5L6888o_"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF_WP6wS88pF"
      },
      "source": [
        "#### Step 2. run cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bOQaaDb88pG"
      },
      "source": [
        "scores = cross_val_score(clf, iris_train_features, iris_train_labels, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN6sU7zq88pQ"
      },
      "source": [
        "`cv=10` specified that we perform 10-fold cross validation. the function returns a 10 element array, where each element is the accuracy of that fold. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLEB0zdP88pS"
      },
      "source": [
        "print(scores)\n",
        "print(\"The average accuracy is %5.3f\" % (scores.mean()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DY8nsmW88pY"
      },
      "source": [
        "So `scores` contains the accuracy for each of the 10 runs. In my case it was:\n",
        "\n",
        "```\n",
        "[1.         0.83333333 1.         0.91666667 0.91666667 0.91666667\n",
        " 0.91666667 0.91666667 1.         0.91666667]\n",
        "The average accuracy is 0.933\n",
        "```\n",
        "So the best runs were 100% accurate and the worst was 83%. The average accuracy was 93%\n",
        "\n",
        "## You try\n",
        "We have covered a lot of material and now is your chance to practice it using the Pima Indians Diabetes Data we used before. The data file is at \n",
        "\n",
        "[https://raw.githubusercontent.com/zacharski/ml-class/master/data/pima-indians-diabetes.csv](https://raw.githubusercontent.com/zacharski/ml-class/master/data/pima-indians-diabetes.csv)\n",
        "\n",
        "The data file does not contain a header row. Of course you can name the columns whatever you want, but I used:\n",
        "```\n",
        "['pregnant', 'glucose', 'bp', 'skinfold', 'insulin', 'bmi', 'pedigree', 'age', 'diabetes']\n",
        "```\n",
        "\n",
        "### Load in the data file\n",
        "So load in the data file and let's reserve 20% for `pima_test` and 80% for `pima_train`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_rNfFvo88pY"
      },
      "source": [
        "pima = pd.read_csv('https://raw.githubusercontent.com/zacharski/ml-class/master/data/pima-indians-diabetes.csv',names = ['pregnant','glucose','bp','skinfold','insulin','bmi','pedigree','age','diabetes'])\n",
        "\n",
        "pima_train, pima_test = train_test_split(pima, test_size = 0.2)\n",
        "pima_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVQd14uj88pf"
      },
      "source": [
        "### creating separate data structures for the features and labels\n",
        "\n",
        "Next, for convenience let's create 2 DataFrames and 2 Series. The DataFrames are:\n",
        "\n",
        "* `pima_train_features` will contain the feature columns from `pima_train` \n",
        "* `pima_test_features` will contain the feature columns from `pima_test`\n",
        "\n",
        "The Series are:\n",
        "\n",
        "* `pima_train_labels` will contain the `diabetes` column\n",
        "* `pima_test_labels` will also contain the `diabetes` column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-FelhO88pf"
      },
      "source": [
        "pima_train_labels = pima_train['diabetes']\n",
        "pima_test_labels =pima_test['diabetes']\n",
        "\n",
        "pima_train_features = pima_train.drop('diabetes',axis =1)\n",
        "pima_test_features = pima_test.drop('diabetes',axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-aYR1Az88pk"
      },
      "source": [
        "### Exploring hyperparameters: max_depth\n",
        "We are interested in seeing which has higher accuracy:\n",
        "\n",
        "1. a classifier unconstrained for max_depth \n",
        "2. a classifier with max_depth of 4\n",
        "\n",
        "Create 2 decision tree classifiers: `clf` which is unconstrained for depth and `clf4` which has a max_depth of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSRFuVfu88pm"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
        "clf4 = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F6EeSNv88pq"
      },
      "source": [
        "### using 10-fold cross validation get the average accuracy of `clf`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiHgQPdn88pr"
      },
      "source": [
        "scores = cross_val_score(clf, pima_train_features, pima_train_labels, cv=10)\n",
        "print(scores)\n",
        "print(\"The average accuracy is %5.3f\" % (scores.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns4HJLce88pu"
      },
      "source": [
        "### using 10-fold cross validation get the average accuracy of `clf4`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fbNUq9X88pv"
      },
      "source": [
        "scores = cross_val_score(clf4, pima_train_features, pima_train_labels, cv=10)\n",
        "print(scores)\n",
        "print(\"The average accuracy is %5.3f\" % (scores.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUuKRZiC88pz"
      },
      "source": [
        "## which has better accuracy, the one unconstrained for depth or the one whose max_depth is 4?\n",
        " The max depth 4 one has better accuracy\n",
        "\n",
        "### Using the entire training set, train a new classifier with the best setting for the max_depth hyperparameter\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8dB4cua88pz"
      },
      "source": [
        "\n",
        "\n",
        "best_score = 0\n",
        "depth_best = 0\n",
        "for i in range(1,20):\n",
        "  clf = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=i)\n",
        "  scores = cross_val_score(clf, pima_train_features, pima_train_labels, cv=10)\n",
        "  accuracy = (scores.mean())\n",
        "  if (accuracy > best_score):\n",
        "    best_score = accuracy\n",
        "    depth_best = i\n",
        "  else:\n",
        "    best_score = best_score\n",
        "    depth_best = depth_best\n",
        "\n",
        "print(\"The highest accuracy is \" + (str(best_score)))\n",
        "print(\"With a depth of \" + (str(depth_best)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OPRTs288p4"
      },
      "source": [
        "### Finally, using the test set what is the accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gV-AJisU88p4"
      },
      "source": [
        "clf3 = tree.DecisionTreeClassifier(criterion = 'entropy',max_depth=3)\n",
        "scores = cross_val_score(clf3, pima_test_features, pima_test_labels, cv=10)\n",
        "print(scores)\n",
        "print(\"The average accuracy is %5.3f\" % (scores.mean()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKZWN9uB88p7"
      },
      "source": [
        "## Automation\n",
        "\n",
        "Let's say we want to find the best settings for `max_depth`and we will check out the values, 3, 4, 5, 6, ...12 and the best for `min_samples_split` and we will try 2, 3, 4, 5. That makes 10 values for `max_depth` and 4 for `min_samples_split`. That makes 40 different classifiers and it would be time consuming to do that by hand. Fortunately, we can automate the process using [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV). \n",
        "\n",
        "First we will import the module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XR_43aM88p7"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhe-o2H88qC"
      },
      "source": [
        "Now we are going to specify the values we want to test. For `max_depth` we want 3, 4, 5, 6, ... 12 and for `min_samples_split` we want  2, 3, 4, 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ptla1g7p88qC"
      },
      "source": [
        "hyperparam_grid = [\n",
        "    {'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \n",
        "     'min_samples_split': [2,3,4, 5]}\n",
        "  ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LFLfarr88qH"
      },
      "source": [
        "Next, let's create a decision tree classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHdA9_nc88qH"
      },
      "source": [
        "\n",
        "clf = tree.DecisionTreeClassifier(criterion='entropy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKLeXtiJ88qK"
      },
      "source": [
        "#### now create a grid search object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTL066O788qK"
      },
      "source": [
        "grid_search = GridSearchCV(clf, hyperparam_grid, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P1ysgxy88qO"
      },
      "source": [
        "When we create the object we pass in:\n",
        "\n",
        "* the classifer - in our case `clf`\n",
        "* the Python dictionary containing the hyperparameters we want to evaluate. In our case `hyperparam_grid`\n",
        "* how many bins we are using. In our case 10: `cv=10`\n",
        "\n",
        "#### now perform `fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvXUyVJx88qO"
      },
      "source": [
        "grid_search.fit(pima_train_features, pima_train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vgXN2E88qR"
      },
      "source": [
        "When `grid_search` runs, it creates 40 different classifiers and runs 10-fold cross validation on each of them. We can ask `grid_search` what were the parameters of the classifier with the highest accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9e0UbBb88qS"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIwA_DMf88qg"
      },
      "source": [
        "We can also ask `grid_search` to return the best classifier so we can use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj_uV8tB88qg"
      },
      "source": [
        "predictions = grid_search.best_estimator_.predict(pima_test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZuvX3vp88qi",
        "outputId": "01e3e883-8a26-480d-988a-61b8fc2a6932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(pima_test_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7597402597402597"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99TlGzd988ql"
      },
      "source": [
        "As you can see, grid search is extremely helpful in tuning a classifier to work well with a particular problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvduVjuO88qm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}